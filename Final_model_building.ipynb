{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38229127-8a78-4425-8673-370254920828",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "750dcdcc-56a0-428a-ba4d-e7e5a53429d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install dython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb0dd73c-1cf4-49e3-8e43-b61aeb7ccbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e83f3cd4-6063-41d4-827c-c060e10533b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import Parallel, delayed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de8cfc6-d47c-4a56-bc3a-adaa861ef93b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "print(joblib.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d8ab96-4d65-4485-ae20-668e3b21e32e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "gc.collect()  # Force garbage collection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fffd33-42f0-4abf-8898-59f3410f824a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path =  r'C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/Final_list_of_variables/weekly_stats_demo_cleaned_for_model_v4.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "weekly_stats_cleaned= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9e1d1ed-cf9b-4846-b386-45702e6f8cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats_cleaned['subj'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d70c4-1929-447a-b230-bb33d3ab44cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the subject 'RWRAD_065' from the dataset\n",
    "weekly_stats_cleaned_filtered = weekly_stats_cleaned[weekly_stats_cleaned['subj'] != 'RWRAD_065']\n",
    "\n",
    "# Count the NaN values in each column after removing the subject\n",
    "nan_count_filtered = weekly_stats_cleaned_filtered.isna().sum()\n",
    "\n",
    "# Filter columns that have more than 0 NaN values\n",
    "columns_with_nan_filtered = nan_count_filtered[nan_count_filtered > 0]\n",
    "\n",
    "# Display the count of NaN values for each column with NaNs\n",
    "columns_with_nan_filtered\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2215b62e-7757-40b5-befa-c80bf1138b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(weekly_stats_cleaned_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef92fc0-29a4-47ed-90c1-09bfab517abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd49e74-a86b-495c-8aed-71f106bb3aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the subject 'RWRAD_065' from the dataset\n",
    "weekly_stats_cleaned_filtered = weekly_stats_cleaned[weekly_stats_cleaned['subj'] != 'RWRAD_065']\n",
    "\n",
    "# Count the NaN values in each column after removing the subject\n",
    "nan_count_filtered = weekly_stats_cleaned_filtered.isna().sum()\n",
    "\n",
    "# Filter columns that have more than 0 NaN values\n",
    "columns_with_nan_filtered = nan_count_filtered[nan_count_filtered > 0]\n",
    "\n",
    "# Display the count of NaN values for each column with NaNs\n",
    "columns_with_nan_filtered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "567b4d25-1859-4548-a300-c9c12704f471",
   "metadata": {},
   "source": [
    "## Fill 0 in avg_miles_per_chain and avg_minutes_per_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223b29a6-4b79-4aea-a729-7669c4871890",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats_cleaned.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ffc6140-0fdd-4704-91f8-7eb8f6a9fb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats_cleaned[['avg_miles_per_chain', 'avg_minutes_per_chain']] = weekly_stats_cleaned[['avg_miles_per_chain', 'avg_minutes_per_chain']].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc1e514-150c-4885-bec6-48befef0cf03",
   "metadata": {},
   "source": [
    "## Fill 0 in end trip variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d6ee809-d89f-436f-a0a3-2bc66c6cba04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill NaN values with 0 in the specific columns\n",
    "columns_to_fill = [\n",
    "    'errand_weekend_count', \n",
    "    'home_weekday_count', \n",
    "    'medical_weekday_count', \n",
    "    'none_weekday_count', \n",
    "    'social_weekday_count'\n",
    "]\n",
    "\n",
    "weekly_stats_cleaned[columns_to_fill] = weekly_stats_cleaned[columns_to_fill].fillna(0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6ca644f-b4b4-4155-9ebe-051e7af97544",
   "metadata": {},
   "source": [
    "# Categorical to binary (label encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d72e77aa-4aab-4602-874d-6aff4de48091",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you are modifying the original DataFrame using .loc[]\n",
    "weekly_stats_cleaned.loc[:, 'subj_type'] = weekly_stats_cleaned['subj_type'].apply(lambda x: 1 if x in ['mci', 'alzheimers'] else 0)\n",
    "\n",
    "## mci/alzheimers= 1 and normalaging =0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a8c8e5-3863-41b8-9cc0-bc405d16a6a9",
   "metadata": {},
   "source": [
    "# One hot encode (gender)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ab424-7815-4095-8a3c-992c9bf713d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding using pd.get_dummies for 'gender' and 'race'\n",
    "weekly_stats_cleaned = pd.get_dummies(weekly_stats_cleaned, columns=['gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f640f45d-251a-43f1-97aa-77c78d5a3603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the columns are in int format after transformation\n",
    "#weekly_stats_cleaned['race_white'] = weekly_stats_cleaned['race_white'].astype(int)\n",
    "weekly_stats_cleaned['gender_male'] = weekly_stats_cleaned['gender_male'].astype(int)\n",
    "weekly_stats_cleaned['subj_type'] = weekly_stats_cleaned['subj_type'].astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c0e8cd-fbbe-4580-b6a4-d502d7674492",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekly_stats_cleaned.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63399692-e5d5-4cf4-9e72-f37df4899089",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install joblib\n",
    "\n",
    "from joblib import dump\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b20dc6-0311-4502-af53-a67e04effd55",
   "metadata": {},
   "source": [
    "## XGBoost with LOSO cross validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af54a91-6619-4987-9c8a-5bf66ed381e1",
   "metadata": {},
   "source": [
    "### With parallel processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a523aa-01f0-425a-85a8-3090cc68daf5",
   "metadata": {},
   "source": [
    "#### 1) Using all variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ee7524-6080-46d0-8c6f-8ebdb7a1886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed, dump\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "X = weekly_stats_cleaned.drop(columns=['subj_type', 'subj', 'week_number', 'education_years', 'race', 'year', 'home_weekday_count', 'medical_weekday_count', 'none_weekday_count', 'social_weekday_count'])\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=10,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 10, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1d7a67-3edb-426f-a7a3-16cdb4ab9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_all_var.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_all_var.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ffbb69-4857-4bc5-9129-f0571bc75363",
   "metadata": {},
   "source": [
    "### 2) Using Only demographics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b70f51-5e23-483d-86ac-7b66d34d7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "selected_columns = ['gender_male', 'age', 'education_numberofyears_nacc']\n",
    "X = weekly_stats_cleaned[selected_columns]\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42238b99-0119-40b4-8f01-1f087e1f8ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_demographics.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_demographics.csv\", index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "415e4880-8995-4b5f-af5a-24ba2fc92d69",
   "metadata": {},
   "source": [
    "### 3) Using Only Driving Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f184cf84-a0c2-4abb-b17f-2adf9edb0caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "X = weekly_stats_cleaned.drop(columns=['subj_type', 'subj', 'week_number', 'education_years', 'race', 'year', 'home_weekday_count', 'medical_weekday_count', 'none_weekday_count', 'social_weekday_count', 'gender_male', 'age', 'education_numberofyears_nacc',  'sd_efficiency', 'mean_efficiency', 'mean_tst', 'sd_tst'])\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_driving_var.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_driving_var.csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3107f255-bc87-47cc-aa5a-c0374a48dda3",
   "metadata": {},
   "source": [
    "### 4) Using Only Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c043ccb1-5f69-45a0-8717-464959da13f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "selected_columns = ['sd_efficiency', 'mean_efficiency', 'mean_tst', 'sd_tst']\n",
    "X = weekly_stats_cleaned[selected_columns]\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_sleep.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_sleep.csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fdca558-0022-446f-817f-d6306d1e9dce",
   "metadata": {},
   "source": [
    "### 5) Using Only Demographics and Driving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad7cb3d4-2868-46a2-a436-89f824830357",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "X = weekly_stats_cleaned.drop(columns=['subj_type', 'subj', 'week_number', 'education_years', 'race', 'year', 'home_weekday_count', 'medical_weekday_count', 'none_weekday_count', 'social_weekday_count','sd_efficiency', 'mean_efficiency', 'mean_tst', 'sd_tst'])\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_driving_demo.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_driving_demo).csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991fa93c-f143-4641-ba07-6a4e100fb991",
   "metadata": {},
   "source": [
    "### 6) Using Only Driving and Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cf7c8dc-77f3-48be-b7da-77b63fdd2809",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "X = weekly_stats_cleaned.drop(columns=['subj_type', 'subj', 'week_number', 'education_years', 'race', 'year', 'home_weekday_count', 'medical_weekday_count', 'none_weekday_count', 'social_weekday_count', 'gender_male', 'age', 'education_numberofyears_nacc'])\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_driving_sleep.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_driving_sleep).csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29be719-5596-4020-972b-42902d92218d",
   "metadata": {},
   "source": [
    "## Demographics + Sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f5a6a-6144-42d5-8bcf-d8ffa60f0f5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from joblib import Parallel, delayed\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# Assuming weekly_stats_cleaned is your original dataframe\n",
    "selected_columns = ['gender_male', 'age', 'education_numberofyears_nacc', 'sd_efficiency', 'mean_efficiency', 'mean_tst', 'sd_tst']\n",
    "X = weekly_stats_cleaned[selected_columns]\n",
    "X = X.replace({None: np.nan, '': np.nan})\n",
    "y = weekly_stats_cleaned['subj_type']\n",
    "subjects = weekly_stats_cleaned['subj']\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "cv_splitter = StratifiedGroupKFold(\n",
    "    n_splits=5,\n",
    "    shuffle=True,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Define parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [1, 3, 5],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'reg_alpha': [1, 5, 10],\n",
    "    'reg_lambda': [5, 10, 15],\n",
    "    'subsample': [0.7, 0.8],  # Add row sampling\n",
    "    'colsample_bytree': [0.7, 0.8]  # Add column sampling\n",
    "}\n",
    "\n",
    "\n",
    "def losocv_for_subject(left_out_subject):\n",
    "    \"\"\"Perform LOSOCV with hyperparameter tuning for a given subject.\"\"\"\n",
    "    train_mask = subjects != left_out_subject\n",
    "    test_mask = subjects == left_out_subject\n",
    "\n",
    "    X_train, y_train = X[train_mask], y[train_mask]\n",
    "    X_test, y_test = X[test_mask], y[test_mask]\n",
    "\n",
    "    # IMPORTANT: groups for StratifiedGroupKFold (subject IDs for training rows)\n",
    "    groups_train = subjects[train_mask]\n",
    "\n",
    "    # Handle class imbalance (safe guard)\n",
    "    num_class_0 = (y_train == 0).sum()\n",
    "    num_class_1 = (y_train == 1).sum()\n",
    "    scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "    # Initialize the XGBClassifier\n",
    "    xgb_model = XGBClassifier(\n",
    "        random_state=42,\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\"\n",
    "    )\n",
    "\n",
    "    # Use RandomizedSearchCV for hyperparameter tuning\n",
    "    random_search = RandomizedSearchCV(\n",
    "        estimator=xgb_model,\n",
    "        param_distributions=param_dist,\n",
    "        n_iter=50,\n",
    "        scoring=\"f1\",\n",
    "        cv=cv_splitter,\n",
    "        n_jobs=1,\n",
    "        random_state=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit RandomizedSearchCV on training data (WITH groups)\n",
    "    random_search.fit(X_train, y_train, groups=groups_train)\n",
    "    best_model = random_search.best_estimator_\n",
    "\n",
    "    # Probabilities + predictions\n",
    "    y_prob = best_model.predict_proba(X_test)[:, 1]\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    y_prob_train = best_model.predict_proba(X_train)[:, 1]\n",
    "    y_pred_train = (y_prob_train >= 0.5).astype(int)\n",
    "\n",
    "    # Calculate training and test accuracies\n",
    "    train_accuracy = accuracy_score(y_train, y_pred_train)\n",
    "    test_accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "    # Get feature importances\n",
    "    feature_importances = getattr(best_model, \"feature_importances_\", None)\n",
    "\n",
    "    # Collect results\n",
    "    subject_results = []\n",
    "    for true_label, pred_label, prob, idx in zip(y_test, y_pred, y_prob, X_test.index):\n",
    "        subject_results.append([\n",
    "            left_out_subject,\n",
    "            idx,\n",
    "            int(true_label),\n",
    "            int(pred_label),\n",
    "            float(prob),\n",
    "            float(train_accuracy),\n",
    "            float(test_accuracy),\n",
    "            random_search.best_params_\n",
    "        ])\n",
    "\n",
    "    return subject_results, feature_importances\n",
    "\n",
    "# Run the LOSOCV loop in parallel\n",
    "results = Parallel(n_jobs= 14, verbose=10)(\n",
    "    delayed(losocv_for_subject)(subject) for subject in subjects.unique()\n",
    ")\n",
    "\n",
    "# Separate results and feature importances\n",
    "flattened_results = []\n",
    "all_feature_importances = []\n",
    "for subject_result, feature_importance in results:\n",
    "    flattened_results.extend(subject_result)\n",
    "    if feature_importance is not None:\n",
    "        all_feature_importances.append(feature_importance)\n",
    "\n",
    "# Convert results into a DataFrame\n",
    "losocv_results = pd.DataFrame(flattened_results, columns=[\"Subject\", \"Data Point Index\", \"True Label\", \"Predicted Label\", \"Prob_1\", \"Training Accuracy\", \"Test Accuracy\", \"Best Hyperparameters\"])\n",
    "\n",
    "# Check if we have any feature importances\n",
    "if all_feature_importances:\n",
    "    # Compute mean feature importance across all subjects\n",
    "    feature_names = X.columns\n",
    "    feature_importance_df = pd.DataFrame(all_feature_importances, columns=feature_names)\n",
    "    mean_feature_importance = feature_importance_df.mean().sort_values(ascending=False)\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    mean_feature_importance.plot(kind='bar', color='skyblue')\n",
    "    plt.title(\"Mean Feature Importance Across All Subjects\", fontsize=16)\n",
    "    plt.xlabel(\"Features\", fontsize=14)\n",
    "    plt.ylabel(\"Mean Importance\", fontsize=14)\n",
    "    plt.xticks(rotation=90, ha='right', fontsize=10)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No feature importances were collected. Check if the model supports feature_importances_.\")\n",
    "\n",
    "# Optionally, save results to CSV\n",
    "losocv_results.to_csv(\"losocv_results_with_hyperparameter_tuning_parallel_demo_sleep.csv\", index=False)\n",
    "mean_feature_importance.to_csv(\"mean_feature_importance_demo_sleep.csv\", index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54a0890-a6a1-450b-9ffe-d9f543f34b8f",
   "metadata": {},
   "source": [
    "## Choose best hyperparameters from the losocv results for all feature combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b24a2418-0b43-4be1-9701-5e254d4b11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "\n",
    "folder = r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/LOSO_CV results\"\n",
    "\n",
    "# grab all LOSOCV result csvs in that folder\n",
    "files = sorted(glob.glob(os.path.join(folder, \"losocv_results*.csv\")))\n",
    "\n",
    "print(\"Found files:\")\n",
    "for f in files:\n",
    "    print(\" -\", os.path.basename(f))\n",
    "\n",
    "summary_rows = []\n",
    "\n",
    "for fp in files:\n",
    "    df = pd.read_csv(fp)\n",
    "\n",
    "    # sanity checks\n",
    "    required_cols = {\"Subject\", \"Best Hyperparameters\"}\n",
    "    missing = required_cols - set(df.columns)\n",
    "    if missing:\n",
    "        summary_rows.append({\n",
    "            \"file\": os.path.basename(fp),\n",
    "            \"status\": f\"SKIPPED (missing columns: {sorted(missing)})\",\n",
    "            \"n_subjects\": None,\n",
    "            \"top_hp_count\": None,\n",
    "            \"top_hp_pct\": None,\n",
    "            \"best_hyperparameters\": None\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    # one hyperparameter set per subject (since weekly rows repeat the same dict)\n",
    "    hp_per_subject = df.groupby(\"Subject\")[\"Best Hyperparameters\"].first()\n",
    "\n",
    "    # count frequency of each hyperparameter set\n",
    "    hp_counts = hp_per_subject.value_counts(dropna=False)\n",
    "\n",
    "    best_hp_str = hp_counts.index[0]\n",
    "    best_count = int(hp_counts.iloc[0])\n",
    "    n_subjects = int(hp_per_subject.shape[0])\n",
    "    best_pct = best_count / n_subjects if n_subjects else None\n",
    "\n",
    "    # parse dict safely (in case you want to use it programmatically later)\n",
    "    best_hp_dict = None\n",
    "    try:\n",
    "        best_hp_dict = literal_eval(best_hp_str)  # safer than eval\n",
    "    except Exception:\n",
    "        # keep as string if it can't be parsed cleanly\n",
    "        best_hp_dict = None\n",
    "\n",
    "    summary_rows.append({\n",
    "        \"file\": os.path.basename(fp),\n",
    "        \"status\": \"OK\",\n",
    "        \"n_subjects\": n_subjects,\n",
    "        \"top_hp_count\": best_count,\n",
    "        \"top_hp_pct\": best_pct,\n",
    "        \"best_hyperparameters_str\": best_hp_str,\n",
    "        \"best_hyperparameters_dict\": best_hp_dict\n",
    "    })\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows)\n",
    "\n",
    "# save a summary csv so you have everything in one place\n",
    "out_path = os.path.join(folder, \"best_hyperparameters_summary.csv\")\n",
    "summary_df.to_csv(out_path, index=False)\n",
    "\n",
    "print(\"\\nSaved summary to:\", out_path)\n",
    "\n",
    "# show a compact view\n",
    "display_cols = [\"file\", \"status\", \"n_subjects\", \"top_hp_count\", \"top_hp_pct\"]\n",
    "print(summary_df[display_cols].to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991871d5-f803-4c4f-b02c-84d684cf0a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e41b2c-a728-419e-922a-5e0708e52795",
   "metadata": {},
   "source": [
    "## save the baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60505c46-5cd3-40b3-a305-7697842544cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from ast import literal_eval\n",
    "from xgboost import XGBClassifier\n",
    "from joblib import dump\n",
    "from pathlib import Path\n",
    "\n",
    "# -----------------------\n",
    "# A) Load summary hyperparameters\n",
    "# -----------------------\n",
    "summary_path = r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/LOSO_CV results/best_hyperparameters_summary.csv\"\n",
    "summary_df = pd.read_csv(summary_path)\n",
    "\n",
    "def get_best_hyperparameters(summary_df, losocv_filename):\n",
    "    row = summary_df.loc[summary_df[\"file\"] == losocv_filename]\n",
    "    if row.empty:\n",
    "        raise ValueError(f\"No entry found in summary for file: {losocv_filename}\")\n",
    "    # Try dict column first if present\n",
    "    if \"best_hyperparameters_dict\" in row.columns and pd.notna(row.iloc[0].get(\"best_hyperparameters_dict\", np.nan)):\n",
    "        return literal_eval(row.iloc[0][\"best_hyperparameters_dict\"])\n",
    "    # Otherwise parse from string\n",
    "    return literal_eval(row.iloc[0][\"best_hyperparameters_str\"])\n",
    "\n",
    "# -----------------------\n",
    "# B) Define feature-set combinations (7)\n",
    "# -----------------------\n",
    "LOSOCV_FILE_MAP = {\n",
    "    \"all_var\": \"losocv_results_with_hyperparameter_tuning_parallel_all_var.csv\",\n",
    "    \"demographics\": \"losocv_results_with_hyperparameter_tuning_parallel_demographics.csv\",\n",
    "    \"sleep\": \"losocv_results_with_hyperparameter_tuning_parallel_sleep.csv\",\n",
    "    \"driving_var\": \"losocv_results_with_hyperparameter_tuning_parallel_driving_var.csv\",\n",
    "    \"driving_demo\": \"losocv_results_with_hyperparameter_tuning_parallel_driving_demo.csv\",\n",
    "    \"driving_sleep\": \"losocv_results_with_hyperparameter_tuning_parallel_driving_sleep.csv\",\n",
    "    \"demo_sleep\": \"losocv_results_with_hyperparameter_tuning_parallel_demo_sleep.csv\",\n",
    "}\n",
    "\n",
    "# -----------------------\n",
    "# C) Define columns for each block\n",
    "# -----------------------\n",
    "META_COLS = [\n",
    "    \"subj_type\", \"subj\", \"week_number\", \"year\",\n",
    "    \"education_years\", \"race\",\n",
    "    \"home_weekday_count\", \"medical_weekday_count\", \"none_weekday_count\", \"social_weekday_count\"\n",
    "]\n",
    "\n",
    "DEMO_COLS = [\"gender_male\", \"age\", \"education_numberofyears_nacc\"]\n",
    "SLEEP_COLS = [\"sd_efficiency\", \"mean_efficiency\", \"mean_tst\", \"sd_tst\"]\n",
    "\n",
    "# Helper to build X by \"keeping\" some columns\n",
    "def build_X(weekly_stats_cleaned, keep_cols=None, drop_cols=None):\n",
    "    if keep_cols is not None:\n",
    "        missing = [c for c in keep_cols if c not in weekly_stats_cleaned.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing keep_cols columns: {missing}\")\n",
    "        X = weekly_stats_cleaned[keep_cols].copy()\n",
    "    else:\n",
    "        # drop mode\n",
    "        missing = [c for c in drop_cols if c not in weekly_stats_cleaned.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing drop_cols columns: {missing}\")\n",
    "        X = weekly_stats_cleaned.drop(columns=drop_cols).copy()\n",
    "\n",
    "    X = X.replace({None: np.nan, \"\": np.nan})\n",
    "    # Ensure numeric only (XGBoost needs numeric)\n",
    "    obj_cols = X.columns[X.dtypes == \"object\"].tolist()\n",
    "    if obj_cols:\n",
    "        raise ValueError(f\"Non-numeric columns in X: {obj_cols}. Encode or drop them.\")\n",
    "    return X\n",
    "\n",
    "# Driving features = everything except meta + demographics + sleep\n",
    "DRIVING_DROP_COLS = META_COLS + DEMO_COLS + SLEEP_COLS\n",
    "\n",
    "# -----------------------\n",
    "# D) Prepare y and scale_pos_weight (same for all)\n",
    "# -----------------------\n",
    "y_all = weekly_stats_cleaned[\"subj_type\"].copy()\n",
    "\n",
    "num_class_0 = (y_all == 0).sum()\n",
    "num_class_1 = (y_all == 1).sum()\n",
    "scale_pos_weight = (num_class_0 / num_class_1) if num_class_1 > 0 else 1.0\n",
    "\n",
    "print(\"Baseline rows:\", len(weekly_stats_cleaned))\n",
    "print(\"Class 0:\", int(num_class_0), \"Class 1:\", int(num_class_1))\n",
    "print(\"scale_pos_weight:\", float(scale_pos_weight))\n",
    "\n",
    "# -----------------------\n",
    "# E) Save directory\n",
    "# -----------------------\n",
    "save_dir = Path(r\"C:\\Users\\aparnaj8\\Box\\InTrans\\RWRAD_Internal\\Final_files_with_variables\\LOSO_CV results\\Trained models_classification\")\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# -----------------------\n",
    "# F) Train + save each frozen model\n",
    "# -----------------------\n",
    "trained_models = {}\n",
    "\n",
    "for model_name, losocv_file in LOSOCV_FILE_MAP.items():\n",
    "    best_hp = get_best_hyperparameters(summary_df, losocv_file)\n",
    "\n",
    "    # Build X for each feature set\n",
    "    if model_name == \"all_var\":\n",
    "        # same as your all-var drop list\n",
    "        drop_cols = META_COLS\n",
    "        X_model = build_X(weekly_stats_cleaned, drop_cols=drop_cols)\n",
    "\n",
    "    elif model_name == \"demographics\":\n",
    "        X_model = build_X(weekly_stats_cleaned, keep_cols=DEMO_COLS)\n",
    "\n",
    "    elif model_name == \"sleep\":\n",
    "        X_model = build_X(weekly_stats_cleaned, keep_cols=SLEEP_COLS)\n",
    "\n",
    "    elif model_name == \"driving_var\":\n",
    "        X_model = build_X(weekly_stats_cleaned, drop_cols=DRIVING_DROP_COLS)\n",
    "\n",
    "    elif model_name == \"driving_demo\":\n",
    "        # driving + demographics = everything except meta + sleep\n",
    "        drop_cols = META_COLS + SLEEP_COLS\n",
    "        X_model = build_X(weekly_stats_cleaned, drop_cols=drop_cols)\n",
    "\n",
    "    elif model_name == \"driving_sleep\":\n",
    "        # driving + sleep = everything except meta + demographics\n",
    "        drop_cols = META_COLS + DEMO_COLS\n",
    "        X_model = build_X(weekly_stats_cleaned, drop_cols=drop_cols)\n",
    "\n",
    "    elif model_name == \"demo_sleep\":\n",
    "        # demographics + sleep only\n",
    "        X_model = build_X(weekly_stats_cleaned, keep_cols=DEMO_COLS + SLEEP_COLS)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "    # Train frozen model\n",
    "    model = XGBClassifier(\n",
    "        **best_hp,\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",\n",
    "        missing=np.nan,\n",
    "        scale_pos_weight=scale_pos_weight,\n",
    "        random_state=42,\n",
    "        n_jobs=1,\n",
    "        tree_method=\"hist\",\n",
    "        predictor=\"cpu_predictor\"\n",
    "    )\n",
    "\n",
    "    model.fit(X_model, y_all)\n",
    "\n",
    "    # Save\n",
    "    model_path = save_dir / f\"xgb_frozen_baseline_{model_name}.joblib\"\n",
    "    dump(model, model_path)\n",
    "\n",
    "    # quick prob sanity check\n",
    "    prob = model.predict_proba(X_model)[:, 1]\n",
    "    print(f\"\\n[{model_name}] saved -> {model_path}\")\n",
    "    print(f\"[{model_name}] X shape: {X_model.shape} | prob range: {float(prob.min()):.4f} to {float(prob.max()):.4f}\")\n",
    "\n",
    "    trained_models[model_name] = (model, X_model)\n",
    "\n",
    "print(\"\\nDone. Trained and saved 7 frozen baseline models.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d8030d-7568-4f48-b634-aaaba4443aec",
   "metadata": {},
   "source": [
    "## Future Prediction using these models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9684a94-b841-4a71-9de7-c7daa01ffa5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path =  r'C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/Final_list_of_variables/weekly_stats_demo_cleaned_for_model_future_year.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "merged_data= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbdad317-f004-4ee6-bd9a-206d1d2e8d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9509106-61aa-4aac-a981-2a75e8aad98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where 'subj_type2' is NaN\n",
    "merged_data = merged_data.dropna(subset=['subj_type2'])\n",
    "\n",
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9185a41-d152-4d56-9f4e-ff77118ed5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['gender'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903df2fe-4386-45dd-9d56-88bd9e03d0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['subj'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f1bbe65-1579-4ff7-8873-3ddb91eee193",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.isna().sum().sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326e1179-8624-4b42-9c46-1990f8560ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply one-hot encoding using pd.get_dummies for 'gender' and 'race'\n",
    "merged_data = pd.get_dummies(merged_data, columns=['gender'], drop_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d18ff7-44b3-4f9f-84bc-2c9980542001",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure you are modifying the original DataFrame using .loc[]\n",
    "merged_data.loc[:, 'subj_type2'] = merged_data['subj_type2'].apply(lambda x: 1 if x in ['mci', 'alzheimers'] else 0)\n",
    "\n",
    "## mci/alzheimers= 1 and normalaging =0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd8be97-f995-4670-8598-0ad63cc1365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data[['avg_miles_per_chain', 'avg_minutes_per_chain']] = merged_data[['avg_miles_per_chain', 'avg_minutes_per_chain']].fillna(0)\n",
    "# Fill NaN values with 0 in the specific columns\n",
    "columns_to_fill = [\n",
    "    'errand_weekend_count', \n",
    "    'home_weekday_count', \n",
    "    'medical_weekday_count', \n",
    "    'none_weekday_count', \n",
    "    'social_weekday_count'\n",
    "]\n",
    "\n",
    "merged_data[columns_to_fill] = merged_data[columns_to_fill].fillna(0)\n",
    "\n",
    "#weekly_stats_cleaned['race_white'] = weekly_stats_cleaned['race_white'].astype(int)\n",
    "merged_data['gender_male'] = merged_data['gender_male'].astype(int)\n",
    "merged_data['subj_type2'] = merged_data['subj_type2'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270c17e5-f4f1-4eb0-bfd1-7f6d8fa50296",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f86424ba-4e0b-46a3-ad31-69ff003010af",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data['subj'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16839d4a-4022-48e1-896b-17fe4fccc704",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data.groupby(\"subj_type2\")[\"subj\"].nunique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "987ebbc3-465b-4377-9bd8-8a8530ca549e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count the number of 0s and 1s in 'subj_type2'\n",
    "value_counts = merged_data['subj_type2'].value_counts()\n",
    "\n",
    "print(value_counts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6783d919-2c7b-46e2-b5d4-34548f59a0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"RWRAD_065\" in merged_data[\"subj\"].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b860e0fc-30f2-4b4d-8976-3406717e8dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data = merged_data[merged_data[\"subj\"] != \"RWRAD_065\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae56990-4da6-4b73-a2f4-6d290c24069e",
   "metadata": {},
   "source": [
    "### DO future prediction uisng the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1cc208-5d17-4813-b5d9-c4698cf1d2dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "\n",
    "# =========================\n",
    "# 1) Model paths\n",
    "# =========================\n",
    "MODEL_DIR = Path(\n",
    "    r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/\"\n",
    "    r\"LOSO_CV results/Trained models_classification\"\n",
    ")\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"all_var\": \"xgb_frozen_baseline_all_var.joblib\",\n",
    "    \"demographics\": \"xgb_frozen_baseline_demographics.joblib\",\n",
    "    \"sleep\": \"xgb_frozen_baseline_sleep.joblib\",\n",
    "    \"driving_var\": \"xgb_frozen_baseline_driving_var.joblib\",\n",
    "    \"driving_demo\": \"xgb_frozen_baseline_driving_demo.joblib\",\n",
    "    \"driving_sleep\": \"xgb_frozen_baseline_driving_sleep.joblib\",\n",
    "    \"demo_sleep\": \"xgb_frozen_baseline_demo_sleep.joblib\",\n",
    "}\n",
    "\n",
    "# =========================\n",
    "# 2) Column definitions\n",
    "# =========================\n",
    "META_COLS = [\n",
    "    \"subj\", \"subj_type\", \"subj_type2\",\n",
    "    \"week_number\", \"year\", \"year2\",\n",
    "    \"education_years\", \"race\",\n",
    "    \"home_weekday_count\", \"medical_weekday_count\",\n",
    "    \"none_weekday_count\", \"social_weekday_count\",\n",
    "]\n",
    "\n",
    "DEMO_COLS = [\"gender_male\", \"age\", \"education_numberofyears_nacc\"]\n",
    "SLEEP_COLS = [\"sd_efficiency\", \"mean_efficiency\", \"mean_tst\", \"sd_tst\"]\n",
    "\n",
    "DRIVING_DROP_COLS = META_COLS + DEMO_COLS + SLEEP_COLS\n",
    "\n",
    "\n",
    "def build_X(df, model_name):\n",
    "    \"\"\"Build feature matrix exactly like baseline training.\"\"\"\n",
    "    df = df.copy().replace({None: np.nan, \"\": np.nan})\n",
    "\n",
    "    if model_name == \"all_var\":\n",
    "        X = df.drop(columns=META_COLS)\n",
    "\n",
    "    elif model_name == \"demographics\":\n",
    "        X = df[DEMO_COLS]\n",
    "\n",
    "    elif model_name == \"sleep\":\n",
    "        X = df[SLEEP_COLS]\n",
    "\n",
    "    elif model_name == \"driving_var\":\n",
    "        X = df.drop(columns=DRIVING_DROP_COLS)\n",
    "\n",
    "    elif model_name == \"driving_demo\":\n",
    "        X = df.drop(columns=META_COLS + SLEEP_COLS)\n",
    "\n",
    "    elif model_name == \"driving_sleep\":\n",
    "        X = df.drop(columns=META_COLS + DEMO_COLS)\n",
    "\n",
    "    elif model_name == \"demo_sleep\":\n",
    "        X = df[DEMO_COLS + SLEEP_COLS]\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model_name: {model_name}\")\n",
    "\n",
    "    # Safety check\n",
    "    obj_cols = X.columns[X.dtypes == \"object\"].tolist()\n",
    "    if obj_cols:\n",
    "        raise ValueError(f\"Non-numeric columns in X for {model_name}: {obj_cols}\")\n",
    "\n",
    "    return X\n",
    "\n",
    "\n",
    "def weekly_predictions_for_future(model, X, df, threshold=0.5):\n",
    "    \"\"\"Weekly predictions for follow-up year.\"\"\"\n",
    "    prob = model.predict_proba(X)[:, 1]\n",
    "    pred = (prob >= threshold).astype(int)\n",
    "\n",
    "    return pd.DataFrame({\n",
    "        \"Subject\": df[\"subj\"].values,\n",
    "        \"Week\": df[\"week_number\"].values,\n",
    "        \"True_Label\": df[\"subj_type2\"].astype(int).values,\n",
    "        \"Predicted_Label\": pred.astype(int),\n",
    "        \"Prob_1\": prob.astype(float),\n",
    "    })\n",
    "\n",
    "\n",
    "def subject_ensemble_majority_vote(weekly_df):\n",
    "    \"\"\"\n",
    "    EXACT baseline logic:\n",
    "    Final_call = 1 if (#predicted_1 > #predicted_0) else 0\n",
    "    \"\"\"\n",
    "    subj_df = (\n",
    "        weekly_df\n",
    "        .groupby(\"Subject\", as_index=False)\n",
    "        .agg(\n",
    "            num_data_points=(\"Predicted_Label\", \"size\"),\n",
    "            true_label=(\"True_Label\", \"first\"),\n",
    "            num_predicted_1=(\"Predicted_Label\", lambda x: (x == 1).sum()),\n",
    "            num_predicted_0=(\"Predicted_Label\", lambda x: (x == 0).sum()),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    subj_df[\"Final_call\"] = (subj_df[\"num_predicted_1\"] > subj_df[\"num_predicted_0\"]).astype(int)\n",
    "    subj_df[\"correct_prediction\"] = subj_df[\"Final_call\"] == subj_df[\"true_label\"]\n",
    "    return subj_df\n",
    "\n",
    "\n",
    "def compute_metrics(subj_df):\n",
    "    y_true = subj_df[\"true_label\"].astype(int).values\n",
    "    y_pred = subj_df[\"Final_call\"].astype(int).values\n",
    "\n",
    "    TP = ((y_true == 1) & (y_pred == 1)).sum()\n",
    "    FP = ((y_true == 0) & (y_pred == 1)).sum()\n",
    "    FN = ((y_true == 1) & (y_pred == 0)).sum()\n",
    "    TN = ((y_true == 0) & (y_pred == 0)).sum()\n",
    "\n",
    "    accuracy = (TP + TN) / (TP + TN + FP + FN)\n",
    "    precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
    "    recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
    "    f1 = (\n",
    "        2 * precision * recall / (precision + recall)\n",
    "        if (precision + recall) > 0 else 0\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"TP\": TP,\n",
    "        \"FP\": FP,\n",
    "        \"FN\": FN,\n",
    "        \"TN\": TN,\n",
    "        \"Accuracy\": accuracy,\n",
    "        \"Precision\": precision,\n",
    "        \"Recall\": recall,\n",
    "        \"F1\": f1,\n",
    "    }\n",
    "\n",
    "\n",
    "# =========================\n",
    "# 3) Run future prediction\n",
    "# =========================\n",
    "all_weekly_outputs = {}\n",
    "all_subject_outputs = {}\n",
    "summary_rows = []\n",
    "\n",
    "for model_name, fname in MODEL_MAP.items():\n",
    "    print(f\"\\nRunning future prediction (majority vote): {model_name}\")\n",
    "\n",
    "    model = load(MODEL_DIR / fname)\n",
    "    X_future = build_X(merged_data, model_name)\n",
    "\n",
    "    weekly_df = weekly_predictions_for_future(model, X_future, merged_data)\n",
    "    subj_df = subject_ensemble_majority_vote(weekly_df)\n",
    "\n",
    "    metrics = compute_metrics(subj_df)\n",
    "    metrics[\"Model\"] = model_name\n",
    "    metrics[\"n_subjects\"] = subj_df.shape[0]\n",
    "\n",
    "    all_weekly_outputs[model_name] = weekly_df\n",
    "    all_subject_outputs[model_name] = subj_df\n",
    "    summary_rows.append(metrics)\n",
    "\n",
    "future_summary_df = pd.DataFrame(summary_rows)\n",
    "print(\"\\nFuture prediction summary (subject-level):\")\n",
    "print(future_summary_df)\n",
    "\n",
    "# =========================\n",
    "# 4) Save outputs\n",
    "# =========================\n",
    "out_dir = Path(\n",
    "    r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/Future_prediction_results\"\n",
    ")\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "future_summary_df.to_csv(\n",
    "    out_dir / \"future_prediction_summary_subjectlevel_majorityvote.csv\",\n",
    "    index=False\n",
    ")\n",
    "\n",
    "for model_name in MODEL_MAP:\n",
    "    all_weekly_outputs[model_name].to_csv(\n",
    "        out_dir / f\"future_weekly_predictions_{model_name}.csv\",\n",
    "        index=False\n",
    "    )\n",
    "    all_subject_outputs[model_name].to_csv(\n",
    "        out_dir / f\"future_subject_predictions_{model_name}_majorityvote.csv\",\n",
    "        index=False\n",
    "    )\n",
    "\n",
    "print(f\"\\nSaved all future prediction outputs to:\\n{out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bc11c9c-06d0-4064-a019-a041d4bc48fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5a5a0-020a-4422-b543-8dd295582110",
   "metadata": {},
   "source": [
    "## Few checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f2156c-6835-425b-ba46-f698356c4866",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_dv = build_X(merged_data, \"driving_var\")\n",
    "X_dd = build_X(merged_data, \"driving_demo\")\n",
    "X_ds = build_X(merged_data, \"driving_sleep\")\n",
    "\n",
    "print(\"Shapes:\", X_dv.shape, X_dd.shape, X_ds.shape)\n",
    "print(\"driving_var == driving_demo columns?\", list(X_dv.columns) == list(X_dd.columns))\n",
    "print(\"driving_var == driving_sleep columns?\", list(X_dv.columns) == list(X_ds.columns))\n",
    "\n",
    "print(\"demo-only cols present in driving_demo:\",\n",
    "      sorted(set(X_dd.columns) - set(X_dv.columns))[:20])\n",
    "\n",
    "print(\"sleep-only cols present in driving_sleep:\",\n",
    "      sorted(set(X_ds.columns) - set(X_dv.columns))[:20])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb3f6a2c-9afb-4193-bf41-c689feec02a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Demo columns health\n",
    "print(merged_data[DEMO_COLS].isna().mean())\n",
    "print(merged_data[DEMO_COLS].nunique())\n",
    "\n",
    "# Sleep columns health\n",
    "print(merged_data[SLEEP_COLS].isna().mean())\n",
    "print(merged_data[SLEEP_COLS].nunique())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfc6aa7-ed8e-42d2-9f8a-8d8355383a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "subj_dv = all_subject_outputs[\"driving_var\"][[\"Subject\",\"Final_call\"]].sort_values(\"Subject\").reset_index(drop=True)\n",
    "subj_dd = all_subject_outputs[\"driving_demo\"][[\"Subject\",\"Final_call\"]].sort_values(\"Subject\").reset_index(drop=True)\n",
    "subj_ds = all_subject_outputs[\"driving_sleep\"][[\"Subject\",\"Final_call\"]].sort_values(\"Subject\").reset_index(drop=True)\n",
    "\n",
    "print(\"Final_call identical dv vs dd:\", (subj_dv[\"Final_call\"].values == subj_dd[\"Final_call\"].values).mean())\n",
    "print(\"Final_call identical dv vs ds:\", (subj_dv[\"Final_call\"].values == subj_ds[\"Final_call\"].values).mean())\n",
    "print(\"Final_call identical dd vs ds:\", (subj_dd[\"Final_call\"].values == subj_ds[\"Final_call\"].values).mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19230383-1ab3-4ec3-bb77-c680096b0f11",
   "metadata": {},
   "outputs": [],
   "source": [
    "wk_dv = all_weekly_outputs[\"driving_var\"][[\"Subject\",\"Week\",\"Predicted_Label\",\"Prob_1\"]].sort_values([\"Subject\",\"Week\"]).reset_index(drop=True)\n",
    "wk_dd = all_weekly_outputs[\"driving_demo\"][[\"Subject\",\"Week\",\"Predicted_Label\",\"Prob_1\"]].sort_values([\"Subject\",\"Week\"]).reset_index(drop=True)\n",
    "wk_ds = all_weekly_outputs[\"driving_sleep\"][[\"Subject\",\"Week\",\"Predicted_Label\",\"Prob_1\"]].sort_values([\"Subject\",\"Week\"]).reset_index(drop=True)\n",
    "\n",
    "print(\"Weekly predicted labels identical dv vs dd:\", (wk_dv[\"Predicted_Label\"].values == wk_dd[\"Predicted_Label\"].values).mean())\n",
    "print(\"Weekly predicted labels identical dv vs ds:\", (wk_dv[\"Predicted_Label\"].values == wk_ds[\"Predicted_Label\"].values).mean())\n",
    "\n",
    "print(\"Mean abs prob diff dv vs dd:\", np.mean(np.abs(wk_dv[\"Prob_1\"].values - wk_dd[\"Prob_1\"].values)))\n",
    "print(\"Mean abs prob diff dv vs ds:\", np.mean(np.abs(wk_dv[\"Prob_1\"].values - wk_ds[\"Prob_1\"].values)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38489b1d-9d2e-4c5e-a946-960d77ae8e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name in [\"driving_var\",\"driving_demo\",\"driving_sleep\"]:\n",
    "    model = load(MODEL_DIR / MODEL_MAP[name])\n",
    "    booster = model.get_booster()\n",
    "    print(name, \"n_features:\", model.n_features_in_)\n",
    "    print(name, \"booster features sample:\", booster.feature_names[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5df7f26-3d88-4dbc-be8d-7adaa4726eb8",
   "metadata": {},
   "source": [
    "## Feature Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d984c3-a2ef-42c0-8fba-da2230bd11c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the file path\n",
    "file_path =  r'C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/LOSO_CV results/mean_feature_importance_driving_demo.csv'\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "feature_df= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1acd21-b75c-4368-a7e5-29b669c6e7e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f05f49-7b6a-4058-a51e-e2be111be480",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_df = feature_df.rename(\n",
    "    columns={\"Unnamed: 0\": \"Features\", '0': \"Importance\"}\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c9d3dd-99f9-4190-9c18-bcfddea1231f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Feature name mapping\n",
    "feature_name_mapping = {\n",
    "    'gender_male': 'Sex',\n",
    "    'mean_tst': 'Mean_Total_Sleep_Time',\n",
    "    'sd_tst': 'SD_Total_Sleep_Time',\n",
    "    'age': 'Age',\n",
    "    'avg_trip_accel_x': 'Avg_Acceleration',\n",
    "    'mean_efficiency': 'Mean_SE',\n",
    "    'stoppage_probability': 'Stop_Sign_Prob',\n",
    "    'education_numberofyears_nacc': 'Education_Years',\n",
    "    'percent_within_25_miles': 'Pct_LT_25_Miles',\n",
    "    'sd_efficiency': 'Standard_Deviation_SE',\n",
    "    'avg_miles_per_trip': 'Avg_Miles_Trip',\n",
    "    'trip_chains': 'Trip_Chains',\n",
    "    'within_25_miles': 'Trips_LT_25_Miles',\n",
    "    'avg_trip_speed': 'Avg_Speed',\n",
    "    'within_15_miles': 'Trips_LT_15_Miles',\n",
    "    'avg_minutes_per_trip': 'Avg_Min_Trip',\n",
    "    'hard_brake_events': 'Hard_Brakes',\n",
    "    'percent_pm_4_6': 'Pct_PM_Peak_Trips',\n",
    "    'errand_weekend_count': 'Weekend_Errands',\n",
    "    'total_minutes': 'Tot_Trip_Min',\n",
    "    'avg_miles_per_chain': 'Miles_per_Chain',\n",
    "    'total_trips': 'Tot_Trips',\n",
    "    'avg_minutes_per_chain': 'Min_per_Chain',\n",
    "    'percent_am_7_9': 'Pct_AM_Peak_Trips',\n",
    "    'percent_within_15_miles': 'Pct_LT_15_Miles',\n",
    "    'total_miles': 'Tot_Miles',\n",
    "    'pm_4_6_trips': 'PM_Peak_Trips',\n",
    "    'percent_speed_60_mph': 'Pct_HighSpeed_Trips',\n",
    "    'percent_nighttime_80': 'Pct_Night_80',\n",
    "    'percent_non_nighttime': 'Pct_Non_Night',\n",
    "    'am_7_9_trips': 'AM_Peak_Trips',\n",
    "    'nighttime_80_percent': 'Night_80_Pct',\n",
    "    'non_nighttime_trips': 'Non_Night_Trips',\n",
    "    'speed_60_mph': 'Speed_60_MPH'\n",
    "}\n",
    "\n",
    "# Apply feature name mapping safely\n",
    "feature_df['Features'] = feature_df['Features'].replace(feature_name_mapping)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_df = feature_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Generate a smooth color gradient from a colorblind-friendly palette\n",
    "cmap = sns.color_palette(\"cividis\", as_cmap=True)  # cividis for colorblind-friendly gradient\n",
    "norm = plt.Normalize(feature_df['Importance'].min(), feature_df['Importance'].max())\n",
    "colors = [cmap(norm(value)) for value in feature_df['Importance']]\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "bars = sns.barplot(x='Features', y='Importance', data=feature_df, palette=colors, ax=ax)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Features', fontsize=14)\n",
    "ax.set_ylabel('Feature Importance (Gain)', fontsize=14)\n",
    "ax.set_title('Feature Importance by Gain for Classifier with Multimodal Data', fontsize=16, fontweight='bold')\n",
    "plt.xticks(rotation=90, ha='right', fontsize=12)  # Rotate feature names for better visibility\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)  # Add horizontal grid lines\n",
    "\n",
    "# Add color bar for reference\n",
    "sm = plt.cm.ScalarMappable(cmap=\"cividis\", norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Feature Importance\", fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3054581-dd2d-42d7-8407-c61db53fef2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the plot to a file\n",
    "plot_file_path = \"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/LOSO_CV results/feature_importance_plot_demo_driving_baseline.png\"\n",
    "fig.tight_layout()\n",
    "fig.savefig(plot_file_path)\n",
    "\n",
    "\n",
    "# Return the file path to download\n",
    "plot_file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49936e5-6640-44c6-950b-efc03bcaf7a2",
   "metadata": {},
   "source": [
    "## feature for future prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0532cf5c-8791-4039-bbbb-fd44136e94c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -------------------------\n",
    "# Load the frozen demo_sleep model\n",
    "# -------------------------\n",
    "MODEL_DIR = Path(\n",
    "    r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/\"\n",
    "    r\"LOSO_CV results/Trained models_classification\"\n",
    ")\n",
    "\n",
    "model = load(MODEL_DIR / \"xgb_frozen_baseline_demo_sleep.joblib\")\n",
    "booster = model.get_booster()\n",
    "\n",
    "# -------------------------\n",
    "# Choose importance type\n",
    "# Options: \"gain\", \"weight\", \"cover\", \"total_gain\", \"total_cover\"\n",
    "# \"gain\" is usually what people report.\n",
    "# -------------------------\n",
    "importance_type = \"gain\"\n",
    "\n",
    "score_dict = booster.get_score(importance_type=importance_type)\n",
    "\n",
    "# XGBoost returns feature names like \"f0\", \"f1\" if it didn't store names.\n",
    "# But your model DOES show feature_names in booster, so we map properly.\n",
    "feature_names = booster.feature_names  # should be the real column names\n",
    "\n",
    "# Build dataframe with all features (including zeros if missing from get_score)\n",
    "importance_vals = []\n",
    "for f in feature_names:\n",
    "    importance_vals.append(score_dict.get(f, 0.0))\n",
    "\n",
    "feature_df = pd.DataFrame({\n",
    "    \"Features\": feature_names,\n",
    "    \"Importance\": importance_vals\n",
    "})\n",
    "\n",
    "# Optional: drop zeros (features never used in splits)\n",
    "feature_df = feature_df[feature_df[\"Importance\"] > 0].copy()\n",
    "\n",
    "# -------------------------\n",
    "# Apply your feature name mapping safely\n",
    "# -------------------------\n",
    "feature_name_mapping = {\n",
    "    'gender_male': 'Sex',\n",
    "    'mean_tst': 'Mean_Total_Sleep_Time',\n",
    "    'sd_tst': 'SD_Total_Sleep_Time',\n",
    "    'age': 'Age',\n",
    "    'avg_trip_accel_x': 'Avg_Acceleration',\n",
    "    'mean_efficiency': 'Mean_SE',\n",
    "    'stoppage_probability': 'Stop_Sign_Prob',\n",
    "    'education_numberofyears_nacc': 'Education_Years',\n",
    "    'percent_within_25_miles': 'Pct_LT_25_Miles',\n",
    "    'sd_efficiency': 'Standard_Deviation_SE',\n",
    "    'avg_miles_per_trip': 'Avg_Miles_Trip',\n",
    "    'trip_chains': 'Trip_Chains',\n",
    "    'within_25_miles': 'Trips_LT_25_Miles',\n",
    "    'avg_trip_speed': 'Avg_Speed',\n",
    "    'within_15_miles': 'Trips_LT_15_Miles',\n",
    "    'avg_minutes_per_trip': 'Avg_Min_Trip',\n",
    "    'hard_brake_events': 'Hard_Brakes',\n",
    "    'percent_pm_4_6': 'Pct_PM_Peak_Trips',\n",
    "    'errand_weekend_count': 'Weekend_Errands',\n",
    "    'total_minutes': 'Tot_Trip_Min',\n",
    "    'avg_miles_per_chain': 'Miles_per_Chain',\n",
    "    'total_trips': 'Tot_Trips',\n",
    "    'avg_minutes_per_chain': 'Min_per_Chain',\n",
    "    'percent_am_7_9': 'Pct_AM_Peak_Trips',\n",
    "    'percent_within_15_miles': 'Pct_LT_15_Miles',\n",
    "    'total_miles': 'Tot_Miles',\n",
    "    'pm_4_6_trips': 'PM_Peak_Trips',\n",
    "    'percent_speed_60_mph': 'Pct_HighSpeed_Trips',\n",
    "    'percent_nighttime_80': 'Pct_Night_80',\n",
    "    'percent_non_nighttime': 'Pct_Non_Night',\n",
    "    'am_7_9_trips': 'AM_Peak_Trips',\n",
    "    'nighttime_80_percent': 'Night_80_Pct',\n",
    "    'non_nighttime_trips': 'Non_Night_Trips',\n",
    "    'speed_60_mph': 'Speed_60_MPH'\n",
    "}\n",
    "\n",
    "feature_df[\"Features\"] = feature_df[\"Features\"].replace(feature_name_mapping)\n",
    "\n",
    "# Sort features by importance in descending order\n",
    "feature_df = feature_df.sort_values(by=\"Importance\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "print(feature_df.head(20))\n",
    "\n",
    "# -------------------------\n",
    "# Plot (your style)\n",
    "# -------------------------\n",
    "cmap = sns.color_palette(\"cividis\", as_cmap=True)\n",
    "norm = plt.Normalize(feature_df[\"Importance\"].min(), feature_df[\"Importance\"].max())\n",
    "colors = [cmap(norm(v)) for v in feature_df[\"Importance\"]]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 6))\n",
    "sns.barplot(x=\"Features\", y=\"Importance\", data=feature_df, palette=colors, ax=ax)\n",
    "\n",
    "ax.set_xlabel(\"Features\", fontsize=14)\n",
    "ax.set_ylabel(f\"Feature Importance ({importance_type})\", fontsize=14)\n",
    "ax.set_title(\"Feature Importance (Gain)\", fontsize=16)\n",
    "plt.xticks(rotation=90, ha=\"right\", fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "\n",
    "sm = plt.cm.ScalarMappable(cmap=\"cividis\", norm=norm)\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm, ax=ax)\n",
    "cbar.set_label(\"Feature Importance\", fontsize=12)\n",
    "\n",
    "fig_path = out_path/\"feature_importance_demo_sleep_gain.png\"\n",
    "plt.savefig(fig_path, dpi=300, bbox_inches=\"tight\")\n",
    "\n",
    "plt.show()\n",
    "print(\"Saved figure to:\", fig_path)\n",
    "\n",
    "# Optional: save to csv\n",
    "out_path = Path(r\"C:/Users/aparnaj8/Box/InTrans/RWRAD_Internal/Final_files_with_variables/Future_prediction_results\")\n",
    "out_path.mkdir(parents=True, exist_ok=True)\n",
    "feature_df.to_csv(out_path / f\"feature_importance_demo_sleep_{importance_type}.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d54fdbb6-5af7-40b4-a239-7e08456d143d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
